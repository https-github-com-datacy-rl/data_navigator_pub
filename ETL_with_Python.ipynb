{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02b4f46-dd16-4210-a963-6c0fbe5e2718",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract, Transform and Load (ETL) with Python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9478a704-b4cd-4b68-82e6-1be0526757ab",
   "metadata": {},
   "source": [
    "This notebook will help you perform an ETL with python using data sources from the local folder, PostgresQL and AWS S3 Bucket.\n",
    "\n",
    "Python ETL Tools are the general ETL Tools written in Python and support other Python libraries for extracting, loading, and transforming different types of tables of data imported from multiple data sources like XML, CSV, Text, or JSON, etc into Data Warehouses, Data Lakes, etc. Python is a widely used language to create Data pipelines and is easy to manage. Python ETL Tools are fast, reliable, and deliver high performance\n",
    "\n",
    "Please install the necessary packages before you continue. \n",
    "- ```!pip install pandas psycopg2 sqlalchemy boto3```\n",
    "- The other packages are python inbuilt packages \n",
    "\n",
    "This is the steps in this notebook.\n",
    "- Import packages\n",
    "- Initiate config parser; you will need it later for the credentials\n",
    "- Utility functions\n",
    "- Extract\n",
    "- Transform \n",
    "- Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87f30b8-9cf7-4c71-8d87-3a3b556ec53b",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Import packages\n",
    "\n",
    "Import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f92848b-6e54-4b4b-b0a1-ec503cab67c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import configparser\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "import boto3\n",
    "from urllib import request\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907fccb5-e6e1-4d76-988d-8106e99dd381",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Initiate config parser; you will need it later for the credentials\n",
    "\n",
    "This step is helpful for authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdac4df2-965d-48b4-89e6-74df5c8902f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['postgresql', 'aws_s3', 'weather_api']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "# read the configuration file\n",
    "config.read('../Dataverse/multi_config.ini')\n",
    "\n",
    "# get all the connections\n",
    "config.sections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de99903a-b1a6-4174-866f-3071b8af9fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful \n",
      "\n",
      "The database is \"data_engineering\" and the service_name is \"s3\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Authenticate the Postgres and S3 database by getting the credentials from the config file\n",
    "'''\n",
    "database = config.get('postgresql', 'database')\n",
    "user = config.get('postgresql', 'user')\n",
    "password = config.get('postgresql', 'password')\n",
    "host = config.get('postgresql', 'host')\n",
    "port = config.get('postgresql', 'port')\n",
    "\n",
    "# AWS Credentials\n",
    "service_name = config.get('aws_s3', 'service_name')\n",
    "region_name = config.get('aws_s3', 'region_name')\n",
    "aws_access_key_id = config.get('aws_s3', 'aws_access_key_id')\n",
    "aws_secret_access_key = config.get('aws_s3', 'aws_secret_access_key')\n",
    "s3_bucket = config.get('aws_s3', 's3_bucket')\n",
    "\n",
    "# Weather API Credentials\n",
    "api_key = config.get('weather_api', 'api_key')\n",
    "print(\"Authentication successful \\n\")\n",
    "print(f'The database is \"{database}\" and the service_name is \"{service_name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b375bfb-119c-494c-8a08-51584c8bb6c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Utility functions\n",
    "\n",
    "- contents: Import data from the S3 bucket. Accept the bucket name and key as input and returns the s3 key as an object.\n",
    "- load_local: Import data from the local folder. Accept the file path as input and returns the data as a dataframe.\n",
    "- load_postgres_table: The function save a dataframe as a new table on the postrgesQL database. It appends the data if the table exists.\n",
    "- extract_postgres_table: The function import data to your workspace as a dataframe using SQLAlchemy engine.\n",
    "- extract_psycopg_data: The function import data to your workspace as a dataframe using Psycopg2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "320ede57-07f3-44b6-8bbe-be5c92f1666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract data using the bucket name and key\n",
    "def contents(bucket_name, key):\n",
    "    \"\"\"\n",
    "    Import data from the S3 bucket. \n",
    "    Accept the bucket name and key as input and \n",
    "    returns the s3 key as an object.\n",
    "    \"\"\"\n",
    "    string_io = io.BytesIO()\n",
    "    s3.Object(bucket_name, key).download_fileobj(string_io)\n",
    "    return string_io.getvalue() \n",
    "\n",
    "\n",
    "def extract_local(path):\n",
    "    \"\"\"\n",
    "    Import data from the local folder. \n",
    "    Accept the file path as input and \n",
    "    returns the data as a dataframe.\n",
    "    \"\"\"\n",
    "    local_data = pd.read_csv(path)\n",
    "    return local_data\n",
    "\n",
    "\n",
    "def load_postgres_table(df, table_name):\n",
    "    \"\"\"\n",
    "    The function save a dataframe as a new table on the postrgesQL database. \n",
    "    It appends the data if the table exists.\n",
    "    Input: df - dataframe, table_name - Name of table to be created\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    alchemyEngine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}/{database}', pool_recycle=3600);\n",
    "    postgreSQLConnection = alchemyEngine.connect();\n",
    "    postgreSQLTable = table_name # \"admission_postgres\";\n",
    "\n",
    "    # Use try/except to catch errors if exists\n",
    "    try:\n",
    "        frame = df.to_sql(postgreSQLTable, postgreSQLConnection, if_exists='fail'); #, index=False); #postgres_data\n",
    "    except ValueError as vx:\n",
    "        print(vx)\n",
    "    except Exception as ex:  \n",
    "        print(ex)\n",
    "    else:\n",
    "        print(f'PostgreSQL Table, \"{postgreSQLTable}\", has been created successfully.');\n",
    "    finally:\n",
    "        postgreSQLConnection.close();\n",
    "        \n",
    "        \n",
    "def extract_postgres_data(table):\n",
    "    \"\"\"\n",
    "    The function import data to your workspace as a dataframe.\n",
    "    Input: Name of the table to be created or updated - table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an engine instance\n",
    "        alchemyEngine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}/{database}', pool_recycle=3600);\n",
    "\n",
    "        # Connect to PostgreSQL server\n",
    "        dbConnection = alchemyEngine.connect();\n",
    "\n",
    "        # Read data from PostgreSQL database table and load into a DataFrame instance\n",
    "        sql = f\"select * from \\\"{table}\\\"\"\n",
    "        dataFrame = pd.read_sql(sql, dbConnection);\n",
    "\n",
    "        pd.set_option('display.expand_frame_repr', False);\n",
    "\n",
    "        # Print the DataFrame\n",
    "        #print(dataFrame); \n",
    "\n",
    "        # Close the database connection\n",
    "        #dbConnection.close()\n",
    "        return dataFrame\n",
    "    except (Exception, Error) as error:\n",
    "        print(\"Error while connecting to PostgreSQL\", error)\n",
    "    finally:\n",
    "        if dbConnection:\n",
    "            dbConnection.close()\n",
    "            print(\"PostgreSQL connection is closed\")\n",
    "\n",
    "    \n",
    "def extract_psycopg_data(table):\n",
    "    \"\"\"\n",
    "    UserWarning: pandas only support SQLAlchemy connectable(engine/connection) \n",
    "    ordatabase string URI or sqlite3 DBAPI2 connection other DBAPI2 objects are\n",
    "    not tested. Please consider using SQLAlchemy. \n",
    "    This is just a practice.\n",
    "    The function import data to your workspace as a dataframe.\n",
    "    Input: Name of the table to be created or updated - table\n",
    "    \n",
    "    \"\"\"\n",
    "    import psycopg2\n",
    "    from psycopg2 import Error\n",
    "    try:\n",
    "        # Connect to an existing database\n",
    "        connection = psycopg2.connect(user=user,\n",
    "                                      password=password,\n",
    "                                      host=host,\n",
    "                                      port=port,\n",
    "                                      database=database)\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "        # SQL query to create a new table\n",
    "        query = f\"select * from \\\"{table}\\\"\"\n",
    "        # Execute a command: this creates a new table\n",
    "        df = pd.read_sql_query(query, connection)\n",
    "        # print(df)\n",
    "        connection.commit()\n",
    "        print(\"Table extracted in PostgreSQL \")\n",
    "        return df\n",
    "\n",
    "    except (Exception, Error) as error:\n",
    "        print(\"Error while connecting to PostgreSQL\", error)\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"PostgreSQL connection is closed\")\n",
    "\n",
    "\n",
    "def load_postgres_table(df, table_name):\n",
    "    \"\"\"\n",
    "    This function helps to load data from a postgres table\n",
    "    Appends data if the table exists\n",
    "    Input: df - Dataframe, table name\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    # Start the alchemy engine\n",
    "    alchemyEngine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}/{database}', pool_recycle=3600);\n",
    "    \n",
    "    # Establish the connection\n",
    "    postgreSQLConnection = alchemyEngine.connect();\n",
    "    \n",
    "    # Name the table to be created\n",
    "    postgreSQLTable = table_name # \"admission_postgres\";\n",
    "\n",
    "    # Use try and except to catch errors \n",
    "    try:\n",
    "        # Load data to postgres, if table exist append\n",
    "        frame = df.to_sql(postgreSQLTable, postgreSQLConnection, if_exists='append', index=0); # 'fail'\n",
    "    except ValueError as vx:\n",
    "        print(vx)\n",
    "    except Exception as ex:  \n",
    "        print(ex)\n",
    "    else:\n",
    "        print(f'PostgreSQL Table, \"{postgreSQLTable}\", has been created successfully.');\n",
    "    finally:\n",
    "        postgreSQLConnection.close();\n",
    "        \n",
    "        \n",
    "def extract_postgres_data(table):\n",
    "\n",
    "    # Create an engine instance\n",
    "    alchemyEngine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}/{database}', pool_recycle=3600);\n",
    "\n",
    "    # Connect to PostgreSQL server\n",
    "    dbConnection = alchemyEngine.connect();\n",
    "\n",
    "    # Read data from PostgreSQL database table and load into a DataFrame instance\n",
    "    sql = f\"select * from \\\"{table}\\\"\"\n",
    "    dataFrame = pd.read_sql(sql, dbConnection);\n",
    "\n",
    "    pd.set_option('display.expand_frame_repr', False);\n",
    "\n",
    "    # Print the DataFrame\n",
    "    #print(dataFrame); \n",
    "\n",
    "    # Close the database connection\n",
    "    dbConnection.close()\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa2acc-c5dd-4ccd-9fc1-df84027736cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract\n",
    "\n",
    "The function extract data from multiple sources in batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "422c7435-b18f-4f3d-ada9-6d1097270af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local file path to the the data\n",
    "path = \"../Dataverse/admission_local.csv\"\n",
    "\n",
    "# \"../Assets/[name-of-asset]\"\n",
    "\n",
    "# Start s3 session\n",
    "s3 = boto3.resource(\n",
    "    service_name=service_name,\n",
    "    region_name=region_name,\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "def extract_data():\n",
    "    \"\"\"\n",
    "    Ingest data from local folder, PostgresQL and AWS S3\n",
    "    Input: None \n",
    "    Returns: 3 dataframes - local_data, s3_data, postgres_data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract data from the local folder\n",
    "    local_data = extract_local(path)\n",
    "    \n",
    "    # Extract data from the AWS s3 bucket\n",
    "    x = contents(s3_bucket,'admission_csv_s3.csv') \n",
    "    s3_data = pd.read_csv(io.BytesIO(x))    \n",
    "    \n",
    "    # Extract data from PostgresQL \n",
    "    # SQLAlchemy\n",
    "    postgres_data = extract_postgres_data('admission_postgres')\n",
    "    \n",
    "    # Psycopg2\n",
    "    psycopg_df = extract_psycopg_data('admission_postgres')\n",
    "    \n",
    "    # Return the data as 3 Dataframes\n",
    "    return local_data, s3_data, postgres_data, psycopg_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62cbe871-ff6d-4992-9189-a3ed11a6e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_data, s3_data, postgres_data,  psycopg_df = extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3861973-ea6a-4e50-9467-2073b8a2da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fd75b6-8b3b-43e2-87e0-1c05d553c3aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transform\n",
    "\n",
    "After extracting the data, we’ll go on to the “Transform” phase of the process. \n",
    "\n",
    "This function will merge the three dataframes we created as a result of the extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965c1371-e6ed-4fcb-8c2a-4b81b2002bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data():\n",
    "    \"\"\"\n",
    "    Transform data by merging the 3 dataframe\n",
    "    Input: None \n",
    "    Output: Merged data as a dataframe\n",
    "    \"\"\"\n",
    "    admission_data = s3_data.\\\n",
    "                        merge(postgres_data,on='Serial_Number').\\\n",
    "                        merge(local_data,on='Serial_Number')\n",
    "    return admission_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25ffc191-1a3c-49f0-961b-9702854053f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_data = transform_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ed7de54-6795-4721-87e4-c64633500777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a770aa-e5c9-4a85-ba19-4cd0f3b3cb60",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load\n",
    "\n",
    "It is time to load the data into the data warehouse where it can be used to generate insight. \n",
    "\n",
    "We are saving the pandas dataframe as a CSV in this scenario. \n",
    "\n",
    "We have gone through the steps of extracting and transforming. Finally, loading the data from various sources into a single target file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "174814e9-abd8-43df-8c5d-379f96384a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load the data to \n",
    "    - local file path\n",
    "    - postgresQL\n",
    "    - AWS S3\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save transformed data as a csv to file path \n",
    "    transform_data.to_csv('new_data.csv', index=False)\n",
    "    print(\"Dataframe is saved as CSV in the local folder.\")\n",
    "    \n",
    "    # Save transformed data as a table on postgresQL\n",
    "    load_postgres_table(postgres_data, \"Merged_Data\")\n",
    "    \n",
    "    # Save transformed data as an S3 Object\n",
    "    csv_buffer = StringIO()\n",
    "    transform_data.to_csv(csv_buffer, header=True, index=False)\n",
    "    csv_buffer.seek(0)\n",
    "    \n",
    "    s3.Object(s3_bucket, 'new_data.csv').put(Body=csv_buffer.getvalue())\n",
    "    print(\"Dataframe is saved as CSV in S3 bucket.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4f2eb69-375d-4746-90bb-0399d104eca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab60a6f-41e4-42ba-b1e9-886bb97bf2e4",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00729fe0-9fe2-45b6-8373-b7dd50bc442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(message):\n",
    "    timestamp_format = '%H:%M:%S-%h-%d-%Y' #Hour-Minute-Second-MonthName-Day-Year\n",
    "    now = datetime.now() # get current timestamp\n",
    "    timestamp = now.strftime(timestamp_format)\n",
    "    with open(\"../Dataverse/logfile.txt\",\"a\") as f:\n",
    "        f.write(timestamp + ',' + message + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779711d9-75c9-4eb3-b1ec-0ae21239b1cb",
   "metadata": {},
   "source": [
    "### Running ETL Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a923f1bb-022b-4839-9570-cc5a7fbf91e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile    = \"../Dataverse/logfile.txt\"              \n",
    "\n",
    "log(\"ETL Job Started\")\n",
    "\n",
    "log(\"Extract phase Started\")\n",
    "\n",
    "local_data, s3_data, postgres_data,  psycopg_df = extract_data()\n",
    "\n",
    "log(\"Extract phase Ended\")\n",
    "\n",
    "log(\"Transform phase Started\")\n",
    "\n",
    "transform_data = transform_data()\n",
    "\n",
    "log(\"Transform phase Ended\")\n",
    "\n",
    "log(\"Load phase Started\")\n",
    "\n",
    "load_data()\n",
    "\n",
    "log(\"Load phase Ended\")\n",
    "\n",
    "log(\"ETL Job Ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b588e-375b-4642-8f9a-9368714b6e62",
   "metadata": {},
   "source": [
    "## Extract, Transform and Load (ETL) with Python - API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7825e6-bda0-4cd0-8665-17d72ba9bb90",
   "metadata": {},
   "source": [
    "This remaining part of this notebook will help you perform an ETL with python using an API as a data source.\n",
    "\n",
    "Please get the following ready before you continue;\n",
    "\n",
    "- Please install the necessary packages before you continue. \n",
    "    - ```!pip install pandas psycopg2 sqlalchemy```\n",
    "    - Get your OpenWeatherMap API key here (https://openweathermap.org/appid).\n",
    "\n",
    "This is the steps in this notebook.\n",
    "- Import packages\n",
    "- Initiate config parser; you will need it later for the credentials\n",
    "- Utility function for PostgreSQL\n",
    "- Extract, transform and load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5e907-f3b3-45af-8c94-e14df6419813",
   "metadata": {},
   "source": [
    "## Extract, transform and load\n",
    "\n",
    "##### Download data through an API - OpenWeatherMap API\n",
    "\n",
    "- Bring out your API key, you need it here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f817c16b-5bbb-49a8-b89b-cc8c0f637b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import configparser\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "import io\n",
    "import boto3\n",
    "from urllib import request\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "Retrieve the current weather forecast from OpenWeatherMap. FL#Lat 28.4717 Lon -80.5378 {'lat': 28.4717, 'lon': -80.5378}\n",
    "\"\"\"\n",
    "\n",
    "# lat, lon = 57.1499, 2.0938\n",
    "def get_weather_forecast(coords={'lat': 57.1499, 'lon': 2.0938}):  # default location at Cape Canaveral,\n",
    "    try:  \n",
    "        ### Extract\n",
    "        \n",
    "        # retrieve forecast for specified coordinates\n",
    "        # api_key = config.api_key  # replace with your own OpenWeatherMap API key\n",
    "        api_key = \"857fdc31d974e60756a1a81d06189a5a\"\n",
    "        \n",
    "        url = f'https://api.openweathermap.org/data/2.5/forecast?lat={coords[\"lat\"]}&lon={coords[\"lon\"]}&appid={api_key}&units=metric'\n",
    "        \n",
    "        data = json.load(request.urlopen(url))\n",
    "        \n",
    "        # Uncomment next line to see how the data looks like. We need to select the data of interest.\n",
    "        # print(data)\n",
    "        \n",
    "        ### Transform\n",
    "        # Select city, country and periods in the data list\n",
    "        forecast = {'city': data['city']['name'],  # city name\n",
    "                    'country': data['city']['country'],  # country name\n",
    "                    'periods': list()}  # list to hold forecast data for future periods\n",
    "        \n",
    "        \n",
    "        for period in data['list'][0:23]:  # populate list with next 23 forecast periods\n",
    "            forecast['periods'].append({'timestamp': datetime.datetime.fromtimestamp(period['dt']),\n",
    "                                        'temp': round(period['main']['temp']),\n",
    "                                        'description': period['weather'][0]['description'].title(),\n",
    "                                        'icon': f'http://openweathermap.org/img/wn/{period[\"weather\"][0][\"icon\"]}.png'})\n",
    "        \n",
    "        forecast = pd.DataFrame(forecast['periods'])\n",
    "        #print(forecast['city'])\n",
    "        #print(forecast['country'])\n",
    "        ### Load\n",
    "        \n",
    "        load_postgres_table(forecast, \"weatherdata\")\n",
    "                \n",
    "        return forecast\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaa897e6-ac66-4497-8d1f-922bf5cb35a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL Table, \"weatherdata\", has been created successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>temp</th>\n",
       "      <th>description</th>\n",
       "      <th>icon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-07 19:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>Broken Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/04d.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-07 22:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>Broken Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/04n.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-08 01:00:00</td>\n",
       "      <td>15</td>\n",
       "      <td>Overcast Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/04n.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-08-08 04:00:00</td>\n",
       "      <td>15</td>\n",
       "      <td>Overcast Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/04n.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-08 07:00:00</td>\n",
       "      <td>15</td>\n",
       "      <td>Overcast Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/04d.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-08-08 10:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>Broken Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/04d.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-08-08 13:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>Scattered Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/03d.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-08-08 16:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>Few Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/02d.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-08-08 19:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>Few Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/02d.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-08-08 22:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>Overcast Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/04n.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-08-09 01:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>Broken Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/04n.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-08-09 04:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>Few Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/02n.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-08-09 07:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>Broken Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/04d.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-08-09 10:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>Overcast Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/04d.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-08-09 13:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>Overcast Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/04d.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-08-09 16:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>Overcast Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/04d.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-08-09 19:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>Overcast Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/04d.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-08-09 22:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>Overcast Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/04n.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-08-10 01:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>Broken Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/04n.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022-08-10 04:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>Scattered Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/03n.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2022-08-10 07:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>Scattered Clouds</td>\n",
       "      <td>http://openweathermap.org/img/wn/03d.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022-08-10 10:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>Clear Sky</td>\n",
       "      <td>http://openweathermap.org/img/wn/01d.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022-08-10 13:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>Clear Sky</td>\n",
       "      <td>http://openweathermap.org/img/wn/01d.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  temp       description  \\\n",
       "0  2022-08-07 19:00:00    17     Broken Clouds   \n",
       "1  2022-08-07 22:00:00    16     Broken Clouds   \n",
       "2  2022-08-08 01:00:00    15   Overcast Clouds   \n",
       "3  2022-08-08 04:00:00    15   Overcast Clouds   \n",
       "4  2022-08-08 07:00:00    15   Overcast Clouds   \n",
       "5  2022-08-08 10:00:00    16     Broken Clouds   \n",
       "6  2022-08-08 13:00:00    16  Scattered Clouds   \n",
       "7  2022-08-08 16:00:00    17        Few Clouds   \n",
       "8  2022-08-08 19:00:00    17        Few Clouds   \n",
       "9  2022-08-08 22:00:00    16   Overcast Clouds   \n",
       "10 2022-08-09 01:00:00    16     Broken Clouds   \n",
       "11 2022-08-09 04:00:00    16        Few Clouds   \n",
       "12 2022-08-09 07:00:00    17     Broken Clouds   \n",
       "13 2022-08-09 10:00:00    16   Overcast Clouds   \n",
       "14 2022-08-09 13:00:00    16   Overcast Clouds   \n",
       "15 2022-08-09 16:00:00    17   Overcast Clouds   \n",
       "16 2022-08-09 19:00:00    17   Overcast Clouds   \n",
       "17 2022-08-09 22:00:00    17   Overcast Clouds   \n",
       "18 2022-08-10 01:00:00    17     Broken Clouds   \n",
       "19 2022-08-10 04:00:00    17  Scattered Clouds   \n",
       "20 2022-08-10 07:00:00    17  Scattered Clouds   \n",
       "21 2022-08-10 10:00:00    17         Clear Sky   \n",
       "22 2022-08-10 13:00:00    17         Clear Sky   \n",
       "\n",
       "                                        icon  \n",
       "0   http://openweathermap.org/img/wn/04d.png  \n",
       "1   http://openweathermap.org/img/wn/04n.png  \n",
       "2   http://openweathermap.org/img/wn/04n.png  \n",
       "3   http://openweathermap.org/img/wn/04n.png  \n",
       "4   http://openweathermap.org/img/wn/04d.png  \n",
       "5   http://openweathermap.org/img/wn/04d.png  \n",
       "6   http://openweathermap.org/img/wn/03d.png  \n",
       "7   http://openweathermap.org/img/wn/02d.png  \n",
       "8   http://openweathermap.org/img/wn/02d.png  \n",
       "9   http://openweathermap.org/img/wn/04n.png  \n",
       "10  http://openweathermap.org/img/wn/04n.png  \n",
       "11  http://openweathermap.org/img/wn/02n.png  \n",
       "12  http://openweathermap.org/img/wn/04d.png  \n",
       "13  http://openweathermap.org/img/wn/04d.png  \n",
       "14  http://openweathermap.org/img/wn/04d.png  \n",
       "15  http://openweathermap.org/img/wn/04d.png  \n",
       "16  http://openweathermap.org/img/wn/04d.png  \n",
       "17  http://openweathermap.org/img/wn/04n.png  \n",
       "18  http://openweathermap.org/img/wn/04n.png  \n",
       "19  http://openweathermap.org/img/wn/03n.png  \n",
       "20  http://openweathermap.org/img/wn/03d.png  \n",
       "21  http://openweathermap.org/img/wn/01d.png  \n",
       "22  http://openweathermap.org/img/wn/01d.png  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_weather_forecast(coords={'lat': 57.1499, 'lon': 2.0938})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f12e8a-8378-498d-8b8d-10336bcb2c2a",
   "metadata": {},
   "source": [
    "##### Download data through an API - Get a summary of a random article on Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adaec28f-ae4b-439f-87d1-0ddd501268fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "There is a public access to Wikipedia API. There is no need for a key.\n",
    "\"\"\"\n",
    "\n",
    "def get_wikipedia_article():\n",
    "    \"\"\"\n",
    "    Retrieve the summary extract for a random Wikipedia article.\n",
    "    Returns data as a json object in a list\n",
    "    Transform the data to select what you need.\n",
    "    \"\"\"\n",
    "    try:  # retrieve random Wikipedia article\n",
    "        # Extract\n",
    "        data = json.load(request.urlopen('https://en.wikipedia.org/api/rest_v1/page/random/summary'))\n",
    "        \n",
    "        # The data extracted is transformed and \n",
    "        # Title, summary of random article and the url is loaded \n",
    "        \n",
    "        return {'title': data['title'],\n",
    "                'extract': data['extract'],\n",
    "                'url': data['content_urls']['desktop']['page']}\n",
    "    \n",
    "    # Catch errors if exists\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3b47247-c7a9-447d-bfd6-03cfe5ef05b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing random Wikipedia article retrieval...\n",
      "\n",
      "Herb Kohl\n",
      "<https://en.wikipedia.org/wiki/Herb_Kohl>\n",
      "Herbert H. Kohl is an American businessman and politician. Alongside his brother and father, the Kohl family created the Kohl's department stores chain, of which Kohl went on to be president and CEO. Kohl also served as a United States Senator from Wisconsin from 1989 to 2013 as a member of the Democratic Party. He chose not to seek re-election in 2012 and was succeeded by fellow Democrat Tammy Baldwin. Kohl is also the former owner of the Milwaukee Bucks of the National Basketball Association.\n"
     ]
    }
   ],
   "source": [
    "# test get_wikipedia_article()\n",
    "print('\\nTesting random Wikipedia article retrieval...')\n",
    "\n",
    "article = get_wikipedia_article()\n",
    "if article:\n",
    "    print(f'\\n{article[\"title\"]}\\n<{article[\"url\"]}>\\n{article[\"extract\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619cfa7d-12a8-428e-b0d6-b9ea9da9aec8",
   "metadata": {},
   "source": [
    "##### Download data through an API - Chicago Food Inspection Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bb9cd5b-baed-4f85-90e1-f984b9f2fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Download Chicago food inspection data\n",
    "There is a public access to this API\n",
    "Practice by running this code to download the data\"\"\"\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import bz2\n",
    "\n",
    "# The url for the API\n",
    "url = (\n",
    "    'https://data.cityofchicago.org/api/views/4ijn-s7e5/'\n",
    "    'rows.csv?accessType=DOWNLOAD'\n",
    ")\n",
    "\n",
    "# The data is downloaded as a CSV file and saved in your \n",
    "# project's folder\n",
    "\n",
    "with bz2.open('../Dataverse/food.csv.bz2', 'w') as out, urlopen(url) as resp:\n",
    "    for i, line in enumerate(resp):\n",
    "        if i > 3001:\n",
    "            break\n",
    "        out.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fad424-8988-4507-98f8-48738fa6ed78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_eng]",
   "language": "python",
   "name": "conda-env-data_eng-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
